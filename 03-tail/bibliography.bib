
@misc{sitg_nuages_nodate,
	title = {Nuages {De} {Points} {Lidar} 2009 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2009-06},
	urldate = {2025-06-23},
	author = {SITG},
}

@misc{bekuzarov_xmem_2023,
	title = {{XMem}++: {Production}-level {Video} {Segmentation} {From} {Few} {Annotated} {Frames}},
	shorttitle = {{XMem}++},
	url = {http://arxiv.org/abs/2307.15958},
	doi = {10.48550/arXiv.2307.15958},
	abstract = {Despite advancements in user-guided video segmentation, extracting complex objects consistently for highly complex scenes is still a labor-intensive task, especially for production. It is not uncommon that a majority of frames need to be annotated. We introduce a novel semi-supervised video object segmentation (SSVOS) model, XMem++, that improves existing memory-based models, with a permanent memory module. Most existing methods focus on single frame annotations, while our approach can effectively handle multiple user-selected frames with varying appearances of the same object or region. Our method can extract highly consistent results while keeping the required number of frame annotations low. We further introduce an iterative and attention-based frame suggestion mechanism, which computes the next best frame for annotation. Our method is real-time and does not require retraining after each user input. We also introduce a new dataset, PUMaVOS, which covers new challenging use cases not found in previous benchmarks. We demonstrate SOTA performance on challenging (partial and multi-class) segmentation scenarios as well as long videos, while ensuring significantly fewer frame annotations than any existing method. Project page: https://max810.github.io/xmem2-project-page/},
	urldate = {2025-06-23},
	publisher = {arXiv},
	author = {Bekuzarov, Maksym and Bermudez, Ariana and Lee, Joon-Young and Li, Hao},
	month = aug,
	year = {2023},
	note = {arXiv:2307.15958 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{cheng_putting_2024,
	title = {Putting the {Object} {Back} into {Video} {Object} {Segmentation}},
	url = {http://arxiv.org/abs/2310.12982},
	doi = {10.48550/arXiv.2310.12982},
	abstract = {We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it interacts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J\&F over XMem with a similar running time and improves by 4.2 J\&F over DeAOT while being three times faster. Code is available at: https://hkchengrex.github.io/Cutie},
	urldate = {2025-06-23},
	publisher = {arXiv},
	author = {Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Lee, Joon-Young and Schwing, Alexander},
	month = apr,
	year = {2024},
	note = {arXiv:2310.12982 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cheng_hkchengrexcutie_2025,
	title = {hkchengrex/{Cutie}},
	copyright = {MIT},
	url = {https://github.com/hkchengrex/Cutie},
	abstract = {[CVPR 2024 Highlight] Putting the Object Back Into Video Object Segmentation},
	urldate = {2025-06-23},
	author = {Cheng, Rex},
	month = jun,
	year = {2025},
	note = {original-date: 2023-10-19T17:49:24Z},
	keywords = {computer-vision, cvpr2024, deep-learning, pytorch, segmentation, video-editing, video-object-segmentation, video-segmentation},
}

@misc{noauthor_mbzuai-metaversexmem2_2025,
	title = {mbzuai-metaverse/{XMem2}},
	copyright = {GPL-3.0},
	url = {https://github.com/mbzuai-metaverse/XMem2},
	abstract = {A tool for efficient semi-supervised video object segmentation (great results with minimal manual labor) and a dataset for benchmarking},
	urldate = {2025-06-23},
	publisher = {mbzuai-metaverse},
	month = jun,
	year = {2025},
	note = {original-date: 2022-11-16T08:46:44Z},
	keywords = {annotation-tool, computer-vision, dataset, deep-learning, interactive-segmentation, segmentation, video-object-segmentation},
}

@misc{noauthor_davis_nodate,
	title = {{DAVIS}: {Densely} {Annotated} {VIdeo} {Segmentation}},
	url = {https://davischallenge.org/},
	urldate = {2025-06-23},
}

@misc{noauthor_mose_nodate,
	title = {{MOSE}: {Complex} {Video} {Object} {Segmentation} {Dataset}},
	shorttitle = {{MOSE}},
	url = {https://henghuiding.github.io/MOSE},
	abstract = {MOSE: Complex Video Object Segmentation Dataset},
	urldate = {2025-06-23},
}

@misc{sofiiuk_reviving_2021,
	title = {Reviving {Iterative} {Training} with {Mask} {Guidance} for {Interactive} {Segmentation}},
	url = {http://arxiv.org/abs/2102.06583},
	doi = {10.48550/arXiv.2102.06583},
	abstract = {Recent works on click-based interactive segmentation have demonstrated state-of-the-art results by using various inference-time optimization schemes. These methods are considerably more computationally expensive compared to feedforward approaches, as they require performing backward passes through a network during inference and are hard to deploy on mobile frameworks that usually support only forward passes. In this paper, we extensively evaluate various design choices for interactive segmentation and discover that new state-of-the-art results can be obtained without any additional optimization schemes. Thus, we propose a simple feedforward model for click-based interactive segmentation that employs the segmentation masks from previous steps. It allows not only to segment an entirely new object, but also to start with an external mask and correct it. When analyzing the performance of models trained on different datasets, we observe that the choice of a training dataset greatly impacts the quality of interactive segmentation. We find that the models trained on a combination of COCO and LVIS with diverse and high-quality annotations show performance superior to all existing models. The code and trained models are available at https://github.com/saic-vul/ritm\_interactive\_segmentation.},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Sofiiuk, Konstantin and Petrov, Ilia A. and Konushin, Anton},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06583 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fair_segment_nodate,
	title = {Segment {Anything} {\textbar} {Meta} {AI}},
	url = {https://segment-anything.com/},
	language = {en},
	urldate = {2025-06-22},
	author = {FAIR},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	doi = {10.48550/arXiv.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{ravi_sam_2024,
	title = {{SAM} 2: {Segment} {Anything} in {Images} and {Videos}},
	shorttitle = {{SAM} 2},
	url = {http://arxiv.org/abs/2408.00714},
	doi = {10.48550/arXiv.2408.00714},
	abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollár, Piotr and Feichtenhofer, Christoph},
	month = oct,
	year = {2024},
	note = {arXiv:2408.00714 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{kasmi_crowdsourced_2022,
	title = {A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata},
	url = {https://zenodo.org/records/7358126},
	doi = {10.5281/zenodo.7358126},
	abstract = {Photovoltaic (PV) energy generation plays a crucial role in the energy transition. Small-scale, residential PV installations are deployed at an unprecedented pace, and their safe integration into the grid necessitates up-to-date, high-quality information. Overhead imagery is increasingly used to improve the knowledge of residential PV installations with machine learning models capable of automatically mapping these installations. However, these models cannot be reliably transferred from one region or imagery source to another without incurring a decrease in accuracy. To address this issue, known as distribution shift, and foster the development of PV array mapping pipelines, we propose a dataset containing aerial images, segmentation masks, and installation metadata. We provide installation metadata for more than 28000 installations. We provide ground truth segmentation masks for 13000 installations, including 7000 with annotations for two different image providers. Finally, we provide installation metadata that matches the annotation for more than 8000 installations. Dataset applications include end-to-end PV registry construction, robust PV installations mapping, and analysis of crowdsourced datasets.},
	urldate = {2025-03-16},
	publisher = {Zenodo},
	author = {Kasmi, Gabriel and Saint-Drenan, Yves-Marie and Trebosc, David and Jolivet, Raphaël and Leloux, Johnathan and Sarr, Babacar and Dubus, Laurent},
	month = jul,
	year = {2022},
	keywords = {computer vision, crowdsourcing, dataset, photovoltaic installations, photovoltaic installations metadata, training dataset},
}

@misc{noauthor_quick_nodate,
	title = {Quick {Start} — {Segmentation} {Models} documentation},
	url = {https://smp.readthedocs.io/en/latest/quickstart.html},
	urldate = {2025-06-17},
}

@misc{khanam_review_2025,
	title = {A {Review} of {YOLOv12}: {Attention}-{Based} {Enhancements} vs. {Previous} {Versions}},
	shorttitle = {A {Review} of {YOLOv12}},
	url = {http://arxiv.org/abs/2504.11995},
	doi = {10.48550/arXiv.2504.11995},
	abstract = {The YOLO (You Only Look Once) series has been a leading framework in real-time object detection, consistently improving the balance between speed and accuracy. However, integrating attention mechanisms into YOLO has been challenging due to their high computational overhead. YOLOv12 introduces a novel approach that successfully incorporates attention-based enhancements while preserving real-time performance. This paper provides a comprehensive review of YOLOv12's architectural innovations, including Area Attention for computationally efficient self-attention, Residual Efficient Layer Aggregation Networks for improved feature aggregation, and FlashAttention for optimized memory access. Additionally, we benchmark YOLOv12 against prior YOLO versions and competing object detectors, analyzing its improvements in accuracy, inference speed, and computational efficiency. Through this analysis, we demonstrate how YOLOv12 advances real-time object detection by refining the latency-accuracy trade-off and optimizing computational resources.},
	urldate = {2025-06-21},
	publisher = {arXiv},
	author = {Khanam, Rahima and Hussain, Muhammad},
	month = apr,
	year = {2025},
	note = {arXiv:2504.11995 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_detrs_2024,
	title = {{DETRs} {Beat} {YOLOs} on {Real}-time {Object} {Detection}},
	url = {http://arxiv.org/abs/2304.08069},
	doi = {10.48550/arXiv.2304.08069},
	abstract = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1\% / 54.3\% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2\% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3\% / 56.2\% AP. The project page: https://zhao-yian.github.io/RTDETR.},
	urldate = {2025-06-21},
	publisher = {arXiv},
	author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
	month = apr,
	year = {2024},
	note = {arXiv:2304.08069 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	urldate = {2025-06-21},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
	note = {arXiv:2205.14135 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{coco_coco_nodate,
	title = {{COCO} - {Common} {Objects} in {Context}},
	url = {https://cocodataset.org/#home},
	urldate = {2025-06-21},
	author = {COCO},
}

@misc{khanam_yolov11_2024,
	title = {{YOLOv11}: {An} {Overview} of the {Key} {Architectural} {Enhancements}},
	shorttitle = {{YOLOv11}},
	url = {http://arxiv.org/abs/2410.17725},
	doi = {10.48550/arXiv.2410.17725},
	abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
	urldate = {2025-06-20},
	publisher = {arXiv},
	author = {Khanam, Rahima and Hussain, Muhammad},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17725 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ultralytics_yolo_nodate,
	title = {{YOLO} {Data} {Augmentation}},
	url = {https://docs.ultralytics.com/guides/yolo-data-augmentation},
	abstract = {Learn about essential data augmentation techniques in Ultralytics YOLO. Explore various transformations, their impacts, and how to implement them effectively for improved model performance.},
	language = {en},
	urldate = {2025-06-19},
	author = {Ultralytics},
}

@misc{tian_yolov12_2025,
	title = {{YOLOv12}: {Attention}-{Centric} {Real}-{Time} {Object} {Detectors}},
	shorttitle = {{YOLOv12}},
	url = {http://arxiv.org/abs/2502.12524},
	doi = {10.48550/arXiv.2502.12524},
	abstract = {Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6\% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1\%/1.2\% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42\% faster, using only 36\% of the computation and 45\% of the parameters. More comparisons are shown in Figure 1.},
	urldate = {2025-06-18},
	publisher = {arXiv},
	author = {Tian, Yunjie and Ye, Qixiang and Doermann, David},
	month = feb,
	year = {2025},
	note = {arXiv:2502.12524 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{albumentations_documentation_nodate,
	title = {Documentation},
	url = {https://albumentations.ai/},
	abstract = {Comprehensive documentation for the Albumentations library},
	language = {en},
	urldate = {2025-06-18},
	author = {Albumentations},
}

@misc{albumentations_albumentations_nodate,
	title = {Albumentations: fast and flexible image augmentations},
	shorttitle = {Albumentations},
	url = {https://albumentations.ai/},
	abstract = {Improve computer vision models with Albumentations, the fast and flexible Python library for high-performance image augmentation. Supports images, masks, bounding boxes, keypoints \& easy framework integration.},
	language = {en},
	urldate = {2025-06-17},
	journal = {Albumentations},
	author = {Albumentations},
}

@misc{ranftl_vision_2021,
	title = {Vision {Transformers} for {Dense} {Prediction}},
	url = {http://arxiv.org/abs/2103.13413},
	doi = {10.48550/arXiv.2103.13413},
	abstract = {We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28\% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02\% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Ranftl, René and Bochkovskiy, Alexey and Koltun, Vladlen},
	month = mar,
	year = {2021},
	note = {arXiv:2103.13413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xie_segformer_2021,
	title = {{SegFormer}: {Simple} and {Efficient} {Design} for {Semantic} {Segmentation} with {Transformers}},
	shorttitle = {{SegFormer}},
	url = {http://arxiv.org/abs/2105.15203},
	doi = {10.48550/arXiv.2105.15203},
	abstract = {We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
	month = oct,
	year = {2021},
	note = {arXiv:2105.15203 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{xiao_unified_2018,
	title = {Unified {Perceptual} {Parsing} for {Scene} {Understanding}},
	url = {http://arxiv.org/abs/1807.10221},
	doi = {10.48550/arXiv.1807.10221},
	abstract = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at {\textbackslash}url\{https://github.com/CSAILVision/unifiedparsing\}.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
	month = jul,
	year = {2018},
	note = {arXiv:1807.10221 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{liu_path_2018,
	title = {Path {Aggregation} {Network} for {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/1803.01534},
	doi = {10.48550/arXiv.1803.01534},
	abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
	month = sep,
	year = {2018},
	note = {arXiv:1803.01534 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{fan_ma-net_2020,
	title = {{MA}-{Net}: {A} {Multi}-{Scale} {Attention} {Network} for {Liver} and {Tumor} {Segmentation}},
	volume = {8},
	shorttitle = {{MA}-{Net}},
	doi = {10.1109/ACCESS.2020.3025372},
	abstract = {Automatic assessing the location and extent of liver and liver tumor is critical for radiologists, diagnosis and the clinical process. In recent years, a large number of variants of U-Net based on Multi-scale feature fusion are proposed to improve the segmentation performance for medical image segmentation. Unlike the previous works which extract the context information of medical image via applying the multi-scale feature fusion, we propose a novel network named Multi-scale Attention Net (MA-Net) by introducing self-attention mechanism into our method to adaptively integrate local features with their global dependencies. The MA-Net can capture rich contextual dependencies based on the attention mechanism. We design two blocks: Position-wise Attention Block (PAB) and Multi-scale Fusion Attention Block (MFAB). The PAB is used to model the feature interdependencies in spatial dimensions, which capture the spatial dependencies between pixels in a global view. In addition, the MFAB is to capture the channel dependencies between any feature map by multi-scale semantic feature fusion. We evaluate our method on the dataset of MICCAI 2017 LiTS Challenge. The proposed method achieves better performance than other state-of-the-art methods. The Dice values of liver and tumors segmentation are 0.960 ± 0.03 and 0.749 ± 0.08 respectively.},
	journal = {IEEE Access},
	author = {Fan, Tongle and Wang, Guanglei and Li, Yan and Wang, Hongrui},
	month = jan,
	year = {2020},
	pages = {179656--179665},
}

@inproceedings{chaurasia_linknet_2017,
	title = {{LinkNet}: {Exploiting} {Encoder} {Representations} for {Efficient} {Semantic} {Segmentation}},
	shorttitle = {{LinkNet}},
	url = {http://arxiv.org/abs/1707.03718},
	doi = {10.1109/VCIP.2017.8305148},
	abstract = {Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3x640x360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.},
	urldate = {2025-06-17},
	booktitle = {2017 {IEEE} {Visual} {Communications} and {Image} {Processing} ({VCIP})},
	author = {Chaurasia, Abhishek and Culurciello, Eugenio},
	month = dec,
	year = {2017},
	note = {arXiv:1707.03718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {1--4},
}

@misc{chen_encoder-decoder_2018,
	title = {Encoder-{Decoder} with {Atrous} {Separable} {Convolution} for {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	doi = {10.48550/arXiv.1802.02611},
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	month = aug,
	year = {2018},
	note = {arXiv:1802.02611 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_pyramid_2017,
	title = {Pyramid {Scene} {Parsing} {Network}},
	url = {http://arxiv.org/abs/1612.01105},
	doi = {10.48550/arXiv.1612.01105},
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	month = apr,
	year = {2017},
	note = {arXiv:1612.01105 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	doi = {10.48550/arXiv.1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = apr,
	year = {2017},
	note = {arXiv:1612.03144 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhou_unet_2018,
	title = {{UNet}++: {A} {Nested} {U}-{Net} {Architecture} for {Medical} {Image} {Segmentation}},
	shorttitle = {{UNet}++},
	url = {http://arxiv.org/abs/1807.10165},
	doi = {10.48550/arXiv.1807.10165},
	abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	month = jul,
	year = {2018},
	note = {arXiv:1807.10165 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@misc{noauthor_imagenet_nodate,
	title = {{ImageNet}},
	url = {https://image-net.org/},
	urldate = {2025-06-17},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to {Segmentation} {Models}’s documentation! — {Segmentation} {Models} documentation},
	url = {https://segmentation-models-pytorch.readthedocs.io/en/latest/index.html},
	urldate = {2025-06-17},
}

@misc{vivian_guide_2023,
	title = {Guide des composants de toits en pente : les différentes parties du toit d'une maison - {IKO}},
	shorttitle = {Guide des composants de toits en pente},
	url = {https://www.iko.com/blog/fr/roof-components-the-parts-that-make-a-roof/},
	abstract = {Le toit d'une maison est fait de plusieurs couches de matériaux, des fermes jusqu'aux bardeaux. Ses bords sont munis de différents éléments qui empêchent l'eau d'endommager les côtés de la maison. Ce guide vous fera découvrir l'anatomie d'une toiture, ainsi que les termes utilisés en architecture pour décrire les différents éléments d’un toit.},
	language = {fr},
	urldate = {2025-06-12},
	journal = {IKO Content Hub},
	author = {vivian},
	month = jul,
	year = {2023},
}

@misc{chen_focalclick_2022,
	title = {{FocalClick}: {Towards} {Practical} {Interactive} {Image} {Segmentation}},
	shorttitle = {{FocalClick}},
	url = {http://arxiv.org/abs/2204.02574},
	doi = {10.48550/arXiv.2204.02574},
	abstract = {Interactive segmentation allows users to extract target masks by making positive/negative clicks. Although explored by many previous works, there is still a gap between academic approaches and industrial needs: first, existing models are not efficient enough to work on low power devices; second, they perform poorly when used to refine preexisting masks as they could not avoid destroying the correct part. FocalClick solves both issues at once by predicting and updating the mask in localized areas. For higher efficiency, we decompose the slow prediction on the entire image into two fast inferences on small crops: a coarse segmentation on the Target Crop, and a local refinement on the Focus Crop. To make the model work with preexisting masks, we formulate a sub-task termed Interactive Mask Correction, and propose Progressive Merge as the solution. Progressive Merge exploits morphological information to decide where to preserve and where to update, enabling users to refine any preexisting mask effectively. FocalClick achieves competitive results against SOTA methods with significantly smaller FLOPs. It also shows significant superiority when making corrections on preexisting masks. Code and data will be released at github.com/XavierCHEN34/ClickSEG},
	urldate = {2025-06-12},
	publisher = {arXiv},
	author = {Chen, Xi and Zhao, Zhiyan and Zhang, Yilei and Duan, Manni and Qi, Donglian and Zhao, Hengshuang},
	month = apr,
	year = {2022},
	note = {arXiv:2204.02574 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chen_conditional_2021,
	title = {Conditional {Diffusion} for {Interactive} {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/9709986},
	doi = {10.1109/ICCV48922.2021.00725},
	abstract = {In click-based interactive segmentation, the mask extraction process is dictated by positive/negative user clicks; however, most existing methods do not fully exploit the user cues, requiring excessive numbers of clicks for satisfactory results. We propose Conditional Diffusion Network (CDNet), which propagates labeled representations from clicks to conditioned destinations with two levels of affinities: Feature Diffusion Module (FDM) spreads features from clicks to potential target regions with global similarity; Pixel Diffusion Module (PDM) diffuses the predicted logits of clicks within locally connected regions. Thus, the information inferred by user clicks could be generalized to proper destinations. In addition, we put forward Diversified Training (DT), which reduces the optimization ambiguity caused by click simulation. With FDM,PDM and DT, CDNet could better understand user’s intentions and make better predictions with limited interactions. CDNet achieves state-of-the-art performance on several benchmarks.},
	urldate = {2025-06-12},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Xi and Zhao, Zhiyan and Yu, Feiwu and Zhang, Yilei and Duan, Manni},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Benchmark testing, Computer vision, Feature extraction, Frequency division multiplexing, Optimization, Segmentation, Training, Transfer/Low-shot/Semi/Unsupervised Learning, Vision applications and systems, grouping and shape},
	pages = {7325--7334},
}

@misc{supervisely_supervisely_nodate,
	title = {Supervisely: {Curate}, {Label} and {Build} {Production} {Models} in {One} {Platform}},
	url = {https://supervisely.com/},
	urldate = {2025-06-12},
	author = {Supervisely},
}

@misc{roboflow_roboflow_nodate,
	title = {Roboflow: {Computer} vision tools for developers and enterprises},
	shorttitle = {Roboflow},
	url = {https://roboflow.com},
	abstract = {Everything you need to build and deploy computer vision models, from automated annotation tools to high-performance deployment solutions.},
	language = {en},
	urldate = {2025-06-12},
	author = {Roboflow},
}

@misc{label_studio_label_nodate,
	title = {Label {Studio} {Documentation} — {Integrate} {Label} {Studio} into your machine learning pipeline},
	url = {https://labelstud.io/guide/ml},
	abstract = {Machine learning frameworks for integrating your model development pipeline seamlessly with your data labeling workflow.},
	language = {en},
	urldate = {2025-06-12},
	author = {Label Studio},
}

@misc{label_studio_open_nodate,
	title = {Open {Source} {Data} {Labeling}},
	url = {https://labelstud.io/},
	abstract = {A flexible data labeling tool for all data types. Prepare training data for computer vision, natural language processing, speech, voice, and video models.},
	language = {en},
	urldate = {2025-06-12},
	journal = {Label Studio},
	author = {Label Studio},
}

@misc{noauthor_shapely_nodate,
	title = {The {Shapely} {User} {Manual} — {Shapely} 2.1.1 documentation},
	url = {https://shapely.readthedocs.io/en/stable/manual.html},
	urldate = {2025-06-10},
}

@misc{noauthor_rasterio_nodate,
	title = {Rasterio: access to geospatial raster data — rasterio 1.5.0.dev documentation},
	url = {https://rasterio.readthedocs.io/en/latest/index.html},
	urldate = {2025-06-10},
}

@misc{noauthor_parquet_nodate,
	title = {Parquet},
	url = {https://parquet.apache.org/},
	abstract = {The Apache Parquet Website},
	language = {en},
	urldate = {2025-06-10},
	journal = {Apache Parquet},
}

@misc{noauthor_geoparquet_nodate,
	title = {Geoparquet: geospatial data in parquet},
	url = {https://geoparquet.org/},
	urldate = {2025-06-10},
}

@misc{cern_home_nodate,
	title = {Home {\textbar} {CERN}},
	url = {https://home.cern/},
	urldate = {2025-06-08},
	author = {CERN},
}

@misc{noauthor_geopandas_nodate,
	title = {{GeoPandas} 1.1.0 — {GeoPandas} 1.1.0+0.gc36eba0.dirty documentation},
	url = {https://geopandas.org/en/stable/},
	urldate = {2025-06-08},
}

@misc{mckinney_pandas_nodate,
	title = {pandas - {Python} {Data} {Analysis} {Library}},
	url = {https://pandas.pydata.org/},
	urldate = {2025-06-08},
	author = {McKinney, Wes},
}

@misc{sitg_commande_nodate,
	title = {Commande de données volumineuses {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/ressources/commande-donnees-volumineuses},
	urldate = {2025-06-08},
	author = {SITG},
}

@misc{office_federal_de_la_statistique_egidewid_nodate,
	title = {{EGID}/{EWID}},
	url = {https://www.bfs.admin.ch/content/bfs/fr/home/registres/registre-personnes/harmonisation-registres/egid-ewid.html},
	language = {fr},
	urldate = {2025-06-05},
	author = {Office fédéral de la statistique},
}

@techreport{etat_de_geneve_inventaire_2025,
	title = {Inventaire des campagnes d'acquisition d'images aériennes du {Canton} de {Genève}},
	author = {Etat de Genève},
	year = {2025},
}

@misc{kirillov_panoptic_2019,
	title = {Panoptic {Segmentation}},
	url = {http://arxiv.org/abs/1801.00868},
	doi = {10.48550/arXiv.1801.00868},
	abstract = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.},
	urldate = {2025-06-03},
	publisher = {arXiv},
	author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollár, Piotr},
	month = apr,
	year = {2019},
	note = {arXiv:1801.00868 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mechea_panoptic_2019,
	title = {Panoptic {Segmentation} — {The} {Panoptic} {Quality} {Metric}.},
	url = {https://medium.com/@danielmechea/panoptic-segmentation-the-panoptic-quality-metric-d69a6c3ace30},
	abstract = {In the previous article, I gave an explanation about various computer vision tasks, ending with Panoptic Segmentation.},
	language = {en},
	urldate = {2025-06-03},
	journal = {Medium},
	author = {Mechea, Daniel},
	month = feb,
	year = {2019},
}

@misc{rosebrock_intersection_2016,
	title = {Intersection over {Union} ({IoU}) for object detection},
	url = {https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/},
	abstract = {Discover how to apply the Intersection over Union metric (Python code included) to evaluate custom object detectors.},
	language = {en-US},
	urldate = {2025-06-03},
	journal = {PyImageSearch},
	author = {Rosebrock, Adrian},
	month = nov,
	year = {2016},
}

@article{jung_benchmarking_2022,
	title = {Benchmarking {Deep} {Learning} {Models} for {Instance} {Segmentation}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/17/8856},
	doi = {10.3390/app12178856},
	abstract = {Instance segmentation has gained attention in various computer vision fields, such as autonomous driving, drone control, and sports analysis. Recently, many successful models have been developed, which can be classified into two categories: accuracy- and speed-focused. Accuracy and inference time are important for real-time applications of this task. However, these models just present inference time measured on different hardware, which makes their comparison difficult. This study is the first to evaluate and compare the performances of state-of-the-art instance segmentation models by focusing on their inference time in a fixed experimental environment. For precise comparison, the test hardware and environment should be identical; hence, we present the accuracy and speed of the models in a fixed hardware environment for quantitative and qualitative analyses. Although speed-focused models run in real-time on high-end GPUs, there is a trade-off between speed and accuracy when the computing power is insufficient. The experimental results show that a feature pyramid network structure may be considered when designing a real-time model, and a balance between the speed and accuracy must be achieved for real-time application.},
	language = {en},
	number = {17},
	urldate = {2025-06-02},
	journal = {Applied Sciences},
	author = {Jung, Sunguk and Heo, Hyeonbeom and Park, Sangheon and Jung, Sung-Uk and Lee, Kyungjae},
	month = jan,
	year = {2022},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural networks, deep learning, image segmentation, instance segmentation, object detection},
	pages = {8856},
}

@misc{ultralytics_classer_nodate,
	title = {Classer},
	url = {https://docs.ultralytics.com/fr/tasks/classify},
	abstract = {Maîtrisez la classification d'images à l'aide de YOLO11. Apprenez à former, valider, prédire et exporter des modèles de manière efficace.},
	language = {fr},
	urldate = {2025-06-02},
	author = {Ultralytics},
}

@misc{sitg_nuages_2019,
	title = {Nuages de points {LiDAR} 2019 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2019-03},
	urldate = {2025-05-29},
	author = {SITG},
	year = {2019},
}

@misc{sitg_nuages_2023,
	title = {Nuages de points {LiDAR} 2023 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2023-03},
	urldate = {2025-05-31},
	author = {SITG},
	year = {2023},
}

@misc{sitg_nuages_2021,
	title = {Nuages de points {LiDAR} 2021 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2021},
	urldate = {2025-05-31},
	author = {SITG},
	year = {2021},
}

@misc{swisstopo_acquisition_2024,
	title = {Acquisition de données {LiDAR}},
	url = {https://www.swisstopo.admin.ch/fr/donnees-lidar-swisstopo},
	abstract = {En 2024 swisstopo a lancé un nouveau relevé LiDAR de la Suisse et de la Principauté du Liechtenstein.   Le laser scanning aéroporté est l’une des approches les plus efficaces pour mesurer le territoire en 3D. Elle offre des données précieuses et de qualité aussi bien en termes de niveau de détail que de précision. Formant la base des modèles numériques de terrain, ces données contribuent aussi à d’autres produits et servent à de nombreuses applications.},
	urldate = {2025-05-31},
	author = {Swisstopo},
	month = aug,
	year = {2024},
}

@misc{cadden_lidar_2021,
	title = {Lidar - comment fonctionne la technologie ?},
	url = {https://www.cadden.fr/fonctionnement-technologie-lidar/},
	abstract = {Lidar : définition, fonctionnement, comparatif Lidar 2D vs Lidar 3D, Lidar vs radar, applications... toutes les bases pour comprendre cette technologie.},
	language = {fr-FR},
	urldate = {2025-05-31},
	author = {CADDEN},
	month = nov,
	year = {2021},
	note = {Section: Zoom sur},
}

@misc{sitg_chiffre-cle_2025,
	title = {Chiffre-clé {SITG}},
	url = {https://ge.ch/sitg/geodata/SITG/STATS_SITG/chiffre_cles_geneve.html},
	urldate = {2025-05-31},
	author = {SITG},
	month = may,
	year = {2025},
}

@misc{swisstopo_swissimage_nodate,
	title = {{SWISSIMAGE} 10 cm},
	url = {https://www.swisstopo.admin.ch/fr/orthophotos-swissimage-10-cm},
	abstract = {La mosaïque d'orthophotos SWISSIMAGE 10 cm est un assemblage des nouvelles images aériennes numériques en couleurs sur l'ensemble de la Suisse avec une résolution au sol de 10 cm dans les régions de plaine et les principales vallées alpines et de 25 cm dans les Alpes. Elle est mise à jour selon un cycle de 3 ans.},
	urldate = {2025-05-31},
	author = {Swisstopo},
}

@misc{stdl_recherche_2024,
	address = {Berne},
	title = {De la recherche à la mise en production: cas d'usage},
	url = {https://backend.swisstopo.admin.ch/fileservice/sdweb-docs-prod-swisstopoch-files/files/2024/03/11/64cd5353-2007-4a18-a86c-f67dd8ee2fd9.pdf},
	urldate = {2025-05-31},
	author = {STDL},
	month = aug,
	year = {2024},
}

@misc{barrette_different_2022,
	title = {The {Different} {Types} of {Ortho} {Images} {Created} in {ArcGIS}},
	url = {https://www.esri.com/arcgis-blog/products/arcgisrealitystudio/imagery/the-different-types-of-ortho-images-created-in-arcgis},
	abstract = {Understand the different types of ortho images created with Reality Mapping in ArcGIS and the advantages to each of them.},
	language = {en-US},
	urldate = {2025-05-31},
	journal = {ArcGIS Blog},
	author = {Barrette, REanne},
	month = oct,
	year = {2022},
}

@misc{krizhevsky_cifar-10_nodate,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2025-05-31},
	author = {Krizhevsky, Alex},
}

@article{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	author = {Krizhevsky, Alex},
	month = aug,
	year = {2009},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2025-05-31},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
}

@misc{noauthor_ogc_nodate,
	title = {{OGC}® {GeoPackage} {Encoding} {Standard}},
	url = {https://www.geopackage.org/spec/},
	urldate = {2025-05-31},
}

@misc{sitg_communes_nodate,
	title = {Communes genevoises {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/cad-commune},
	urldate = {2025-05-31},
	author = {SITG},
}

@misc{sitg_batiments_nodate,
	title = {Bâtiments hors-sol {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/cad-batiment-horsol},
	urldate = {2025-05-31},
	author = {SITG},
}

@misc{sia_sia-shop_nodate,
	title = {{SIA}-{Shop} {Produit}-«{SIA} 380/1 / 2016 {F} - {Besoins} de chaleur pour le chauffage ({Collection} des normes ={\textgreater} {Architecte})»},
	url = {https://shop.sia.ch/collection%20des%20normes/architecte/380-1_2016_f/F/Product},
	abstract = {.,SIA-Shop Produit-«SIA 380/1 / 2016 F - Besoins de chaleur pour le chauffage (Collection des normes ={\textgreater} Architecte)»},
	language = {français},
	urldate = {2025-05-29},
	author = {SIA},
}

@misc{etat_de_geneve_office_nodate,
	title = {Office cantonal de l'agriculture et de la nature ({OCAN})},
	url = {https://www.ge.ch/taxonomy/term/1781},
	abstract = {L'office cantonal de l'agriculture et de la nature (OCAN) a pour mission de restaurer et renforcer l’infrastructure écologique sur l’ensemble du territoire cantonal et transfrontalier et valoriser la dimension paysagère. Il offre des conditions cadres propices à une agriculture productive, dynamique et résiliente; et incite à une alimentation locale et durable. Il promeut la biodiversité et garantit l’intégration de la nature et de l’agriculture dans l’espace urbain; et développe tout particulièrement le patrimoine arboré. Les services de l'OCAN sont situés à deux adresses : Direction de l'Agriculture Chemin du Pont-du-Centenaire 109, 1228 Plan-les-Ouates Téléphone agriculture : +41 22 3887171 agriculture.ocan@etat.ge.ch Direction biodiversité et forêts et Direction arbres et renaturation urbaine Rue des Battoirs 7, 1205 Genève Téléphone nature : +41 22 3885540 nature.ocan@etat.ge.ch},
	language = {fr},
	urldate = {2025-05-29},
	journal = {ge.ch},
	author = {Etat de Genève},
}

@misc{etat_de_geneve_office_nodate-1,
	title = {Office cantonal de l'énergie ({OCEN})},
	url = {https://www.ge.ch/taxonomy/term/1174},
	abstract = {L'office cantonal de l'énergie a pour but de conduire la politique énergétique du canton, notamment en maîtrisant et en réduisant la consommation. Il veille à assurer les conditions d'un approvisionnement durable et fiable en encourageant la production et l'utilisation d'énergies renouvelables et indigènes pour se substituer aux énergies nucléaire et fossile.},
	language = {fr},
	urldate = {2025-05-29},
	journal = {ge.ch},
	author = {Etat de Genève},
}

@misc{stdl_swiss_nodate,
	title = {Le {Swiss} {Territorial} {Data} {Lab}},
	url = {https://www.stdl.ch/},
	abstract = {Service de la géomatique et du registre foncier. Résoudre les problématiques concrètes des administrations publiques en utilisant la science des données appliquée aux géodonnées.},
	language = {fr},
	urldate = {2025-05-29},
	journal = {Le Swiss Territorial Data Lab},
	author = {STDL},
}

@article{wu_samgeo_2023,
	title = {samgeo: {A} {Python} package for segmenting geospatial data with the {Segment} {Anything} {Model} ({SAM})},
	volume = {8},
	issn = {2475-9066},
	shorttitle = {samgeo},
	url = {https://joss.theoj.org/papers/10.21105/joss.05663},
	doi = {10.21105/joss.05663},
	abstract = {Wu et al., (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). Journal of Open Source Software, 8(89), 5663, https://doi.org/10.21105/joss.05663},
	language = {en},
	number = {89},
	urldate = {2025-05-29},
	journal = {Journal of Open Source Software},
	author = {Wu, Qiusheng and Osco, Lucas Prado},
	month = sep,
	year = {2023},
	pages = {5663},
}

@misc{sitg_orthophotos_nodate,
	title = {Orthophotos 2019 (pixel 5 cm) {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/image-aerienne-ortho-2019-05},
	urldate = {2025-05-29},
	author = {SITG},
}

@misc{sitg_toits_nodate,
	title = {Toits des bâtiments {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/cad-batiment-horsol-toit},
	urldate = {2025-05-29},
	author = {SITG},
}

@misc{ign_ign_nodate,
	title = {{IGN} : produire et diffuser les données géographiques et forestières en {France} - {Portail} {IGN} - {IGN}},
	url = {https://www.ign.fr/},
	urldate = {2025-05-29},
	author = {IGN},
}

@misc{etat_de_vaud_lidar_nodate,
	title = {{LiDAR} 2019 {\textbar} État de {Vaud}},
	url = {https://www.vd.ch/territoire-et-construction/cadastre-et-geoinformation/geodonnees/altimetrie-lidar/technologie-lidar/lidar-2019},
	abstract = {Méthode d'acquisition LiDAR},
	language = {fr-CH},
	urldate = {2025-05-29},
	author = {Etat de Vaud},
}

@techreport{g2_solaire_liste_nodate,
	title = {Liste de publication {Interreg} {G2} {Solaire}},
	author = {G2 Solaire},
}

@article{desthieux_cadastre_nodate,
	title = {Cadastre solaire du {Grand} {Genève} : {Plateforme} collaborative pour une appropriation et un développement de l’énergie solaire},
	language = {fr},
	author = {Desthieux, Gilles},
}

@techreport{desthieux_cadastre_2022,
	title = {Cadastre solaire du {Canton} de {Vaud}},
	url = {https://www.hesge.ch/hepia/sites/default/files/raetd/documents/rapport_cadastresolaire_vd_vf.pdf},
	urldate = {2025-05-29},
	author = {Desthieux, Gilles},
	month = aug,
	year = {2022},
}

@misc{european_commission_jrc_nodate,
	title = {{JRC} {Photovoltaic} {Geographical} {Information} {System} ({PVGIS}) - {European} {Commission}},
	url = {https://re.jrc.ec.europa.eu/pvg_tools/en/#MR},
	urldate = {2025-02-23},
	author = {European commission},
}

@techreport{planair_etude_nodate,
	title = {Etude de modèles de financements participatifs pour l'énergie solaire},
	language = {fr},
	author = {Planair},
}

@misc{sitg_nuages_2009,
	title = {Nuages {De} {Points} {Lidar} 2009 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2009-06},
	urldate = {2025-05-29},
	author = {SITG},
	year = {2009},
}

@misc{sitg_nuages_2017,
	title = {Nuages de points {LiDAR} 2017 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2017-02},
	urldate = {2025-05-29},
	author = {SITG},
	year = {2017},
}

@misc{sia_sia_nodate,
	title = {la sia {\textbar} sia {\textbar} section internationale},
	url = {https://www.int.sia.ch/fr/la-sia},
	abstract = {La Société suisse des ingénieurs et des architectes SIA est l’association professionnelle de référence des spécialistes de la construction, des techniques et de l’environnement. Avec ses quelque quinze mille membres actifs dans tous les domaines de l’architecture et de l’ingénierie, la SIA représente un réseau interdisciplinaire unique dont l’objectif central est de façonner le cadre naturel et bâti en Suisse selon des critères de durabilité et de qualité élevés.\&\#13;\&\#13;Instruments professionnels indispensables\&\#13;\&\#13;La SIA et ses membres représentent la culture, la qualité et la compétence en matière de construction. Une réputation notamment liée à l’importante collection de normes éditées par la SIA. Celle-ci élabore, tient à jour et publie en effet un grand nombre de normes, règlements, directives, recommandations et documents divers, qui sont indispensables à la branche suisse de la construction. Quelque 200 commissions sont chargées du développement et du suivi de cette collection.\&\#13;\&\#13;\&\#13;\&\#13;Défense de la culture du bâti\&\#13;\&\#13;Comme association professionnelle active sur le terrain, la SIA défend la culture du bâti en Suisse et les intérêts des praticiens qui en sont responsables. La Société se positionne sur la gestion des ressources, l'énergie et les défis climatiques; elle contribue à définir le développement territorial de la Suisse, établit des priorités en matière de formation et se bat pour une passation des marchés conforme aux réalités professionnelles. Elle s’engage au niveau politique, diffuse avec constance ses valeurs auprès des médias, propose des cours de formation continue et de perfectionnement, offre des conseils juridiques et organise divers séminaires, congrès et expositions sur les enjeux actuels et à venir pour les disciplines qu’elle représente.\&\#13;\&\#13;\&\#13;\&\#13;Réseau de compétences étendu\&\#13;\&\#13;La SIA se compose des quatre groupes professionnels Architecture, Génie civil, Technique/Industrie et Sol/Air/Eau. Organisée sur un modèle fédéraliste, elle compte 18 sections, qui répercutent ses objectifs à l’échelon local et régional. Enfin, les problématiques liées à des domaines d’expertise particuliers sont traitées au sein des 24 sociétés spécialisées de la SIA.\&\#13;\&\#13;},
	language = {fr},
	urldate = {2025-05-26},
	author = {SIA},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {1939-1471},
	shorttitle = {The perceptron},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Brain, Cognition, Memory, Nervous System},
	pages = {386--408},
}

@misc{gabrielkasmi_gabrielkasmibdappv_2025,
	title = {gabrielkasmi/bdappv},
	url = {https://github.com/gabrielkasmi/bdappv},
	urldate = {2025-03-16},
	author = {gabrielkasmi},
	month = feb,
	year = {2025},
	note = {original-date: 2022-12-23T10:23:37Z},
}

@article{kasmi_crowdsourced_2023,
	title = {A crowdsourced dataset of aerial images with annotated solar photovoltaic arrays and installation metadata},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01951-4},
	doi = {10.1038/s41597-023-01951-4},
	abstract = {Photovoltaic (PV) energy generation plays a crucial role in the energy transition. Small-scale, rooftop PV installations are deployed at an unprecedented pace, and their safe integration into the grid requires up-to-date, high-quality information. Overhead imagery is increasingly being used to improve the knowledge of rooftop PV installations with machine learning models capable of automatically mapping these installations. However, these models cannot be reliably transferred from one region or imagery source to another without incurring a decrease in accuracy. To address this issue, known as distribution shift, and foster the development of PV array mapping pipelines, we propose a dataset containing aerial images, segmentation masks, and installation metadata (i.e., technical characteristics). We provide installation metadata for more than 28000 installations. We supply ground truth segmentation masks for 13000 installations, including 7000 with annotations for two different image providers. Finally, we provide installation metadata that matches the annotation for more than 8000 installations. Dataset applications include end-to-end PV registry construction, robust PV installations mapping, and analysis of crowdsourced datasets.},
	language = {en},
	number = {1},
	urldate = {2025-03-16},
	journal = {Scientific Data},
	author = {Kasmi, Gabriel and Saint-Drenan, Yves-Marie and Trebosc, David and Jolivet, Raphaël and Leloux, Jonathan and Sarr, Babacar and Dubus, Laurent},
	month = jan,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Energy grids and networks, Photovoltaics},
	pages = {59},
}

@misc{thebault_frpv_2025,
	title = {{FRPV} - {Classification} model and training material for the detection of {Rooftop} {Photovoltaic} ({RPV}) systems.},
	url = {https://entrepot.recherche.data.gouv.fr/dataset.xhtml?persistentId=doi:10.57745/CH6QN4},
	doi = {10.57745/CH6QN4},
	abstract = {This dataset contains: A Convolution Neural Network trained to identify rooftop photovoltaic systems The training material, here a set of labelled ...},
	language = {fr},
	urldate = {2025-03-16},
	publisher = {Recherche Data Gouv},
	author = {THEBAULT, Martin and NEROT, Boris},
	month = mar,
	year = {2025},
	keywords = {Computer and Information Science, Engineering},
}

@misc{roboflow_roboflow_nodate-1,
	title = {Roboflow {Universe}: {Computer} {Vision} {Datasets}},
	shorttitle = {Roboflow {Universe}},
	url = {https://universe.roboflow.com/},
	abstract = {Download free, open source datasets and pre-trained computer vision machine learning models.},
	language = {en},
	urldate = {2025-03-16},
	journal = {Roboflow},
	author = {Roboflow},
}

@misc{krapf_tumftmrid_2025,
	title = {{TUMFTM}/{RID}},
	copyright = {LGPL-3.0},
	url = {https://github.com/TUMFTM/RID},
	abstract = {Roof Information Dataset for CV-Based Photovoltaic Potential Assessment},
	urldate = {2025-03-16},
	author = {Krapf, Sebastian and Technology, TUM-Institute of Automotive},
	month = feb,
	year = {2025},
	note = {original-date: 2022-03-02T16:23:20Z},
	keywords = {aerial-images, annotation, computer-vision, dataset, deep-learning, labeling, photovoltaic-potential, remote-sensing, roof-information, roof-segments, roof-superstructures, semantic-segmentation},
}

@misc{krapf_rid_2021,
	title = {{RID} dataset},
	url = {https://dataserv.ub.tum.de/s/m1655470},
	abstract = {Universitätsbibliothek der Technischen Universität München},
	language = {fr},
	urldate = {2025-03-16},
	journal = {Universitätsbibliothek der Technischen Universität München},
	author = {Krapf, Sebastian and Bogenrieder, Lukas and Netzler, Fabian and Balke, Georg},
	year = {2021},
}

@article{thebault_comprehensive_2025,
	title = {A comprehensive building-wise rooftop photovoltaic system detection in heterogeneous urban and rural areas: application to {French} territories},
	volume = {388},
	issn = {0306-2619},
	shorttitle = {A comprehensive building-wise rooftop photovoltaic system detection in heterogeneous urban and rural areas},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261925003605},
	doi = {10.1016/j.apenergy.2025.125630},
	abstract = {With the rapid expansion of Rooftop Photovoltaic (RPV) systems, accurately identifying the location of these installations has become essential for urban planning, grid management, and socio-economic analysis. However, existing European datasets of RPV systems are often limited in both spatial coverage and precision, especially in regions with diverse architectural styles. This study presents a novel methodology for identifying RPV systems by employing a convolutional neural network (CNN) trained on high-resolution aerial imagery and building registry data. Alternatively to traditional tile-based methods, we propose a building-by-building approach, ensuring that each building is individually assessed. The model was trained and validated on five French departments representing a variety of roofing materials and urban typologies. It demonstrates a high correlation between predicted and registered RPV systems, though detection performance varies with roofing materials—achieving better accuracy on tiled roofs than slate roofs. When applied to the entire metropolitan French territory, the model processed images of more than 40 million buildings, identifying approximately 600,000 RPV systems. The results’ accuracy is evaluated, taking into account factors such as data quality and local urban characteristics. All data and the model are publicly available for further research and applications.},
	urldate = {2025-03-16},
	journal = {Applied Energy},
	author = {Thebault, Martin and Nerot, Boris and Govehovitch, Benjamin and Ménézo, Christophe},
	month = jun,
	year = {2025},
	pages = {125630},
}

@article{krapf_ridroof_2022,
	title = {{RID}—{Roof} {Information} {Dataset} for {Computer} {Vision}-{Based} {Photovoltaic} {Potential} {Assessment}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/10/2299},
	doi = {10.3390/rs14102299},
	abstract = {Computer vision has great potential to accelerate the global scale of photovoltaic potential analysis by extracting detailed roof information from high-resolution aerial images, but the lack of existing deep learning datasets is a major barrier. Therefore, we present the Roof Information Dataset for semantic segmentation of roof segments and roof superstructures. We assessed the label quality of initial roof superstructure annotations by conducting an annotation experiment and identified annotator agreements of 0.15–0.70 mean intersection over union, depending on the class. We discuss associated the implications on the training and evaluation of two convolutional neural networks and found that the quality of the prediction behaved similarly to the annotator agreement for most classes. The class photovoltaic module was predicted to be best with a class-specific mean intersection over union of 0.69. By providing the datasets in initial and reviewed versions, we promote a data-centric approach for the semantic segmentation of roof information. Finally, we conducted a photovoltaic potential analysis case study and demonstrated the high impact of roof superstructures as well as the viability of the computer vision approach to increase accuracy. While this paper’s primary use case was roof information extraction for photovoltaic potential analysis, its implications can be transferred to other computer vision applications in remote sensing and beyond.},
	language = {en},
	number = {10},
	urldate = {2025-03-15},
	journal = {Remote Sensing},
	author = {Krapf, Sebastian and Bogenrieder, Lukas and Netzler, Fabian and Balke, Georg and Lienkamp, Markus},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {aerial images, annotation, computer vision, dataset, deep learning, labeling, photovoltaic potential, remote sensing, roof information, roof segments, roof superstructures, semantic segmentation},
	pages = {2299},
}

@article{li_deep_2024,
	title = {Deep learning-based framework for city-scale rooftop solar potential estimation by considering roof superstructures},
	volume = {374},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261924012224},
	doi = {10.1016/j.apenergy.2024.123839},
	abstract = {Solar energy is an environmentally friendly energy source. Identifying suitable rooftops for solar panel installation contributes to not only sustainable energy plans but also carbon neutrality goals. Aerial imagery, bolstered by its growing availability, is a cost-effective data source for rooftop solar potential assessment at large scale. Existing studies generally do not take roof superstructures into account when determining how many solar panels can be installed. This procedure will lead to an overestimation of solar potential. Only several works have considered this issue, but none have devised a network that can simultaneously learn roof orientations and roof superstructures. Therefore, we devise SolarNet+, a novel framework to improve the precision of rooftop solar potential estimation. After implementing SolarNet+ on a benchmark dataset, we find that SolarNet+ outperforms other state-of-the-art approaches in both tasks — roof orientations and roof superstructure segmentation. Moreover, the SolarNet+ framework enables rooftop solar estimation at large-scale applications for investigating the correlation between urban rooftop solar potential and various local climate zone (LCZ) types. The results in the city of Brussels reveal that three specific LCZ urban types exhibit the highest rooftop solar potential efficiency: compact highrise (LCZ1), compact midrise (LCZ2), and heavy industry (LCZ10). The annual photovoltaic potential for these LCZ types is reported as 10.56 GWh/year/km2, 11.77 GWh/year/km2, and 10.70 GWh/year/km2, respectively.},
	urldate = {2025-03-13},
	journal = {Applied Energy},
	author = {Li, Qingyu and Krapf, Sebastian and Mou, Lichao and Shi, Yilei and Zhu, Xiao Xiang},
	month = nov,
	year = {2024},
	keywords = {Convolutional neural network, Local climate zone, Remote sensing, Roof superstructure, Solar potential},
	pages = {123839},
}

@misc{klauser_energie_nodate,
	title = {Energie solaire: {Aptitude} des toitures},
	shorttitle = {Energie solaire},
	url = {https://www.bfe.admin.ch/bfe/fr/home/versorgung/digitalisierung/geoinformation/geodaten/solar/solarenergie-eignung-hausdach.html},
	abstract = {Géodonnées concernant l'énergie solaire, aptitude des toitures publiées par l’Office fédéral de l’énergie.},
	language = {fr},
	urldate = {2025-03-05},
	author = {Klauser, Daniel and Schlegel, Thomas},
}

@misc{bfe_wie_nodate,
	title = {Wie viel {Strom} und {Wärme} kann mein {Dach} produzieren?},
	url = {http://www.sonnendach.ch},
	abstract = {Wie viel Strom und Wärme kann mein Dach produzieren? Finden Sie es heraus mit Sonnendach.ch.},
	urldate = {2025-03-05},
	journal = {Sonnendach.ch},
	author = {BFE, Bundesamt für Energie},
}

@misc{meteotest_wir_2025,
	type = {text/html},
	title = {Wir sind {Meteotest}},
	copyright = {Copyright ©2025 Meteotest.},
	url = {https://meteotest.ch/},
	abstract = {Produkte und Services in den Bereichen Wetter \& Klima, Luftreinhaltung, Sonnen- \& Windenergie, Messtechnik, Geoinformatik, Web \& Software.},
	language = {de},
	urldate = {2025-03-05},
	journal = {Meteotest},
	author = {Meteotest},
	month = mar,
	year = {2025},
	note = {Archive Location: /
Publisher: Meteotest},
}

@misc{sitg_superstructures_nodate,
	title = {Superstructures des toits des bâtiments {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/cad-batiment-horsol-toit-sp},
	urldate = {2025-03-05},
	author = {SITG},
}

@article{zaksek_sky-view_2011,
	title = {Sky-{View} {Factor} as a {Relief} {Visualization} {Technique}},
	volume = {3},
	doi = {10.3390/rs3020398},
	abstract = {Remote sensing has become the most important data source for the digital elevation model (DEM) generation. DEM analyses can be applied in various fields and many of them require appropriate DEM visualization support. Analytical hill-shading is the most frequently used relief visualization technique. Although widely accepted, this method has two major drawbacks: identifying details in deep shades and inability to properly represent linear features lying parallel to the light beam. Several authors have tried to overcome these limitations by changing the position of the light source or by filtering. This paper proposes a new relief visualization technique based on diffuse, rather than direct, illumination. It utilizes the sky-view factor—a parameter corresponding to the portion of visible sky limited by relief. Sky-view factor can be used as a general relief visualization technique to show relief characteristics. In particular, we show that this visualization is a very useful tool in archaeology as it improves the recognition of small scale features from high resolution DEMs.},
	journal = {Remote Sensing},
	author = {Zaksek, Klemen and Oštir, Krištof and Kokalj, Žiga},
	month = feb,
	year = {2011},
}

@misc{google_project_nodate,
	title = {Project {Sunroof}},
	url = {https://sunroof.withgoogle.com/},
	abstract = {Enter a state, county, city, or zip code to see a solar estimate for the area, based on the amount of usable sunlight and roof space.},
	urldate = {2025-03-03},
	author = {Google},
}

@misc{urbio_urbio_nodate,
	title = {Urbio {\textbar} {Pour} les bureaux d'ingénieurs-conseils},
	url = {https://www.urb.io/fr/municipalities},
	abstract = {Urbio permet aux consultants, ingénieurs et experts en énergie de réaliser plus rapidement des mandats de planification et de pré-dimensionnement énergétique, grâce à des cartes intuitives et une IA avancée.},
	language = {fr},
	urldate = {2025-03-03},
	journal = {Urbio},
	author = {Urbio},
}

@misc{picterra_infrastructure_nodate,
	title = {Infrastructure · {Picterra}},
	url = {https://picterra.ch/industries/infrastructure/},
	abstract = {Picterra's geospatial ML platform is used in infrastructure inspection to replace manual surveying, detect cracks \& erosion, monitor vegetation, \& more},
	language = {en-US},
	urldate = {2025-03-03},
	journal = {Picterra},
	author = {Picterra},
}

@misc{solargis_regional_nodate,
	title = {Regional {Solar} {Energy} {Potential} {Study} {\textbar} {Solargis}},
	url = {https://solargis.com/services/regional-solar-energy-potential-study},
	abstract = {With Solargis' Regional Solar Energy Potential Study, you will understand the climate and solar resource of the region to ensure optimal PV plant placement.},
	language = {en},
	urldate = {2025-03-03},
	author = {Solargis},
}

@misc{noauthor_c_2025,
	title = {C++},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=C%2B%2B&oldid=222998267},
	abstract = {C++ est un langage de programmation compilé permettant la programmation sous de multiples paradigmes, dont la programmation procédurale, la programmation orientée objet et la programmation générique. Ses bonnes performances, et sa compatibilité avec le langage C en font un des langages de programmation les plus utilisés dans les applications où la performance est critique.
Créé initialement par Bjarne Stroustrup dans les années 1980, le langage C++ est aujourd'hui normalisé par l'ISO. Sa première normalisation date de 1998 (ISO/CEI 14882:1998), ensuite amendée par l'erratum technique de 2003 (ISO/CEI 14882:2003). Une importante mise à jour a été ratifiée et publiée par l'ISO en septembre 2011 sous le nom de ISO/IEC 14882:2011, ou C++11. Depuis, des mises à jour sont publiées régulièrement : en 2014 (ISO/CEI 14882:2014, ou C++14), en 2017 (ISO/CEI 14882:2017, ou C++17) puis en 2020 (ISO/IEC 14882:2020, ou C++20) et en 2024 (C++23).},
	language = {fr},
	urldate = {2025-03-03},
	journal = {Wikipédia},
	month = feb,
	year = {2025},
	note = {Page Version ID: 222998267},
}

@misc{nvidia_cuda_nodate,
	title = {{CUDA} {Toolkit} - {Free} {Tools} and {Training} {\textbar} {NVIDIA} {Developer}},
	url = {https://developer.nvidia.com/cuda-toolkit},
	urldate = {2025-03-03},
	author = {NVIDIA},
}

@misc{sitg_nuages_2013,
	title = {Nuages de points {LiDAR} 2013 {\textbar} {Catalogue} {SITG}},
	url = {https://sitg.ge.ch/donnees/lidar-aerien-2013},
	urldate = {2025-03-03},
	author = {SITG},
	year = {2013},
}

@article{desthieux_etude_2014,
	title = {Etude pour le compte de l’{Office} cantonal de l’énergie ({OCEN}) et des {Services} industriels genevois ({SIG})},
	url = {https://ge.ch/geodata/SITG/CATALOGUE/INFORMATIONS_COMPLEMENTAIRES/RAPPORT_CADASTRE_SOLAIRE_PHASE_2_2014.pdf},
	language = {fr},
	urldate = {2025-03-03},
	author = {Desthieux, Gilles and Gallinelli, Peter and Camponovo, Reto},
	month = oct,
	year = {2014},
}

@article{desthieux_etude_2011,
	title = {Etude réalisée par hepia, {EPFL} et {Politenico} di {Milano} pour le compte du {Service} de l’énergie-{ScanE} et des {Services} industriels genevois ({SIG})},
	url = {https://ge.ch/geodata/SITG/CATALOGUE/INFORMATIONS_COMPLEMENTAIRES/RAPPORT_CADASTRE_SOLAIRE_PHASE_1_2011.pdf},
	language = {fr},
	urldate = {2025-03-03},
	author = {Desthieux, Gilles and Carneiro, Claudio and Morello, Eugenio},
	month = jan,
	year = {2011},
}

@misc{energie_plus_ensoleillement_2010,
	title = {Ensoleillement},
	url = {https://energieplus-lesite.be/theories/climat8/ensoleillement-d8/},
	abstract = {Le rayonnement solaire

En tant que source d'énergie, l'ensoleillement est un facteur climatique dont on a intérêt à tirer parti (de manière passive, via les ouvertures vitrées, et/ou de manière active pour produire de l’énergie) mais dont on doit aussi},
	language = {fr-FR},
	urldate = {2025-02-22},
	journal = {Energie Plus Le Site},
	author = {Energie Plus},
	month = sep,
	year = {2010},
}

@misc{noauthor_anwendung_nodate,
	title = {Anwendung},
	url = {https://www.swissolar.ch/de/wissen/solartechnologien/photovoltaik/anwendung},
	urldate = {2025-02-23},
}

@article{desthieux_solar_2018,
	title = {Solar {Energy} {Potential} {Assessment} on {Rooftops} and {Facades} in {Large} {Built} {Environments} {Based} on {LiDAR} {Data}, {Image} {Processing}, and {Cloud} {Computing}. {Methodological} {Background}, {Application}, and {Validation} in {Geneva} ({Solar} {Cadaster})},
	volume = {4},
	issn = {2297-3362},
	url = {https://www.frontiersin.org/journals/built-environment/articles/10.3389/fbuil.2018.00014/full},
	doi = {10.3389/fbuil.2018.00014},
	abstract = {{\textless}p{\textgreater}The paper presents the core methodology for assessing solar radiation and energy production on building rooftops and vertical facades (still rarely considered) of the inner-city. This integrated tool is based on the use of LiDAR, 2D and 3D cadastral data. Together with solar radiation and astronomical models, it calculates the global irradiance for a set of points located on roofs, ground, and facades. Although the tool takes simultaneously roofs, ground, and facades, different methods of shadow casting are applied. Shadow casting on rooftops is based on image processing techniques. On the other hand, the assessment on facade involves first to create and interpolate points along the facades and then to implement a point-by-point shadow casting routine. The paper is structured in five parts: (i) state of the art on the use of 3D GIS and automated processes in assessing solar radiation in the built environment, (ii) overview on the methodological framework used in the paper, (iii) detailed presentation of the method proposed for solar modeling and shadow casting, in particular by introducing an innovative approach for modeling the sky view factor (SVF), (iv) demonstration of the solar model introduced in this paper through applications in Geneva’s building roofs (solar cadaster) and facades, (v) validation of the solar model in some Geneva’s spots, focusing especially on two distinct comparisons: solar model versus fisheye catchments on partially inclined surfaces (roof component); solar model versus photovoltaic simulation tool PVSyst on vertical surfaces (facades). Concerning the roof component, validation results emphasize global sensitivity related to the density of light sources on the sky vault to model the SVF. The low dense sky model with 145 light sources gives satisfying results, especially when processing solar cadasters in large urban areas, thus allowing to save computation time. In the case of building facades, introducing weighting factor in SVF calculation leads to outputs close to those obtained by PVSyst. Such good validation results make the proposed model a reliable tool to: (i) automatically process solar cadaster on building rooftops and facades at large urban scales and (ii) support solar energy planning and energy transition policies.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-23},
	journal = {Frontiers in Built Environment},
	author = {Desthieux, Gilles and Carneiro, Claudio and Camponovo, Reto and Ineichen, Pierre and Morello, Eugenio and Boulmier, Anthony and Abdennadher, Nabil and Dervey, Sébastien and Ellert, Christoph},
	month = mar,
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {3D-Urban digital models, Cloud computing, Energy Management, Keywords: Urban solar cadaster, Shadow casting, Sky View Factor},
}

@misc{noauthor_solar_2022,
	title = {Solar map},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Solar_map&oldid=1087433251},
	abstract = {A solar map, in general, is a map of a city, state, country, or any piece of land that illustrates information about how much a certain piece of land, building, or home experiences a certain amount of sunlight. Though solar maps are illustrated in many forms, a solar map essentially records where and to what extent a certain location experiences a certain amount of sunlight or radiation.  It normally combines topographic, meteorological, and sometimes financial data to help scholars or consumers and investors in promoting awareness of the potential of solar power.},
	language = {en},
	urldate = {2025-02-23},
	journal = {Wikipedia},
	month = may,
	year = {2022},
	note = {Page Version ID: 1087433251},
}

@article{thebault_large-scale_2022,
	title = {Large-scale evaluation of the suitability of buildings for photovoltaic integration: {Case} study in {Greater} {Geneva}},
	volume = {316},
	issn = {0306-2619},
	shorttitle = {Large-scale evaluation of the suitability of buildings for photovoltaic integration},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261922005050},
	doi = {10.1016/j.apenergy.2022.119127},
	abstract = {In the context of a rapid and massive deployment or renewable energy and in particular solar photovoltaic, it is necessary to develop methods and tools to guide this deployment. To this end, this work proposes a multicriteria approach for evaluating the suitability of a building to be equipped with photovoltaic (PV) systems (PV suitability). In the present case, technical (roof complexity), economic (payback period), environmental (CO2 reduction), energetic (self-consumption), as well as social (heritage constraint) criteria are considered. These criteria are evaluated for each building of the Greater Geneva Agglomeration (GGA), a cross-border French–Swiss territory of nearly 270 000 buildings. A multicriteria method, ELECTRE TRI, makes it possible to sort these buildings into three categories, A, B, and C, that correspond to “very high,” “high,” and “moderate” PV suitabilities, respectively. Large differences are observed within the 210 municipalities of the GGA since some of them have almost no A-ranked buildings whereas others comprise more than 70\% of these buildings. It is shown that, by prioritizing the A-ranked buildings, almost 50\% of the annual electricity consumption of the Geneva Canton could be produced by PV systems. Finally, the method developed here offers a decision-aiding tool that could be used at a territory scale to achieve energy transition goals in terms of solar PV deployment.},
	urldate = {2025-02-24},
	journal = {Applied Energy},
	author = {Thebault, Martin and Desthieux, Gilles and Castello, Roberto and Berrah, Lamia},
	month = jun,
	year = {2022},
	keywords = {ELECTRE TRI, Energy planning, GIS, Multicriteria decision aiding, PV suitability, Photovoltaic, Urban solar},
	pages = {119127},
}

@misc{noauthor_scopus_nodate,
	title = {Scopus preview - {Scopus} - {Document} details - {Quantification} of the suitable rooftop area for solar panel installation from overhead imagery using convolutional neural networks},
	url = {https://www.scopus.com/record/display.uri?eid=2-s2.0-85120859707&origin=inward&txGid=1a1d57321916837d2dde87fff991e016},
	abstract = {TEST 02 - Elsevier's Scopus, the largest abstract and citation database of peer-reviewed literature. Search and access research from the science, technology, medicine, social sciences and arts and humanities fields.},
	language = {en\_US},
	urldate = {2025-02-24},
	note = {ISSN: 1742-6596},
}

@article{stendardo_gpu-enabled_2020,
	title = {{GPU}-{Enabled} {Shadow} {Casting} for {Solar} {Potential} {Estimation} in {Large} {Urban} {Areas}. {Application} to the {Solar} {Cadaster} of {Greater} {Geneva}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/15/5361},
	doi = {10.3390/app10155361},
	abstract = {In the context of encouraging the development of renewable energy, this paper deals with the description of a software solution for mapping out solar potential in a large scale and in high resolution. We leverage the performance provided by Graphics Processing Units (GPUs) to accelerate shadow casting procedures (used both for direct sunlight exposure and the sky view factor), as well as use off-the-shelf components to compute an average weather pattern for a given area. Application of the approach is presented in the context of the solar cadaster of Greater Geneva (2000 km2). The results show that doing the analysis on a square tile of 3.4 km at a resolution of 0.5 m takes up to two hours, which is better than what we were achieving with the previous work. This shows that GPU-based calculations are highly competitive in the field of solar potential modeling.},
	language = {en},
	number = {15},
	urldate = {2025-02-24},
	journal = {Applied Sciences},
	author = {Stendardo, Nabil and Desthieux, Gilles and Abdennadher, Nabil and Gallinelli, Peter},
	month = jan,
	year = {2020},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GPU, shadow casting, solar cadaster, solar potential modeling},
	pages = {5361},
}

@article{drozd_evaluating_2025,
	title = {Evaluating cities' solar potential using geographic information systems: {A} review},
	volume = {209},
	issn = {1364-0321},
	shorttitle = {Evaluating cities' solar potential using geographic information systems},
	url = {https://www.sciencedirect.com/science/article/pii/S1364032124008384},
	doi = {10.1016/j.rser.2024.115112},
	abstract = {Current trends in the global energy market focus on gradually increasing the share of renewable energy sources in the overall energy mix. In recent years, there has been growing interest within the scientific community in assessing the suitability of cities for implementing solar energy solutions. This work discusses various research directions on the solar potential of urban areas, with a particular focus on the role of Geographic Information System (GIS) tools in support of spatial analyses. The main aim of the study was to update the current state of the research based on the analysis of previous works. An attempt was made to assess the role of GIS in research on the solar potential of cities in the context of the overall investigation process. A total of 201 case studies published between 1999 and 2024 (year to date) were analysed, among which articles from 2019–24 were examined in detail. The analysis revealed a wide variation in the approaches regarding the spatial scale of studies and the sources of key data, such as shading and solar radiation. It was shown that one of the key challenges in current analyses is the lack of universality of the methodologies used, leading to divergent, and sometimes challenging to compare final results. In the research aspect, a global urban solar potential was estimated for cities with more than 1 million inhabitants, which amounted to 33.7 PW h annually.},
	urldate = {2025-02-24},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Drozd, Paweł and Kapica, Jacek and Jurasz, Jakub and Dąbek, Paweł},
	month = mar,
	year = {2025},
	keywords = {GIS, Photovoltaics, Renewable energy, Solar city, Solar potential, Urban development},
	pages = {115112},
}

@article{castello_quantification_2021,
	title = {Quantification of the suitable rooftop area for solar panel installation from overhead imagery using {Convolutional} {Neural} {Networks}},
	volume = {2042},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/2042/1/012002},
	doi = {10.1088/1742-6596/2042/1/012002},
	abstract = {The integration of solar technology in the built environment is realized mainly through rooftop-installed panels. In this paper, we leverage state-of-the-art Machine Learning and computer vision techniques applied on overhead images to provide a geo-localization of the available rooftop surfaces for solar panel installation. We further exploit a 3D building database to associate them to the corresponding roof geometries by means of a geospatial post-processing approach. The stand-alone Convolutional Neural Network used to segment suitable rooftop areas reaches an intersection over union of 64\% and an accuracy of 93\%, while a post-processing step using building database improves the rejection of false positives. The model is applied to a case study area in the canton of Geneva and the results are compared with another recent method used in the literature to derive the realistic available area.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {Journal of Physics: Conference Series},
	author = {Castello, Roberto and Walch, Alina and Attias, Raphaël and Cadei, Riccardo and Jiang, Shasha and Scartezzini, Jean-Louis},
	month = nov,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {012002},
}

@misc{noauthor_grand_nodate,
	title = {Grand {Genève} {\textbar} {Accueil}},
	url = {https://www.grand-geneve.org/},
	abstract = {Accueil Un territoire partagé Actualités Agenda culturel En direct Zoom sur A découvrir aussi UN TERRITOIRE PARTAGÉ Le Grand Genève est un groupement de collectivités publiques locales de part et d’autre de la frontière franco-suisse. Cette agglomération transfrontalière englobe les 117 communes du Pôle métropolitain du Genevois français, les 45 communes du Canton de Genève […]},
	language = {fr-FR},
	urldate = {2025-02-24},
	journal = {Grand Genève},
}

@article{thebault_multicriteria_2020,
	title = {Multicriteria roof sorting for the integration of photovoltaic systems in urban environments},
	volume = {60},
	issn = {2210-6707},
	url = {https://www.sciencedirect.com/science/article/pii/S2210670720304807},
	doi = {10.1016/j.scs.2020.102259},
	abstract = {This work deals with the sorting of the suitability of the roofs for the integration of PV systems, implementable as a GIS (Geographical Information System). Such sorting can provide relevant information to help and guide urban actors further develop solar energy, and therefore contribute to the acceleration of the deployment of this source of energy in cities. Three scenarios of sorting are considered: the energetic, the economic and the multicriteria sorting. The energetic and economic sorting scenarios are actually similar to the great majority of the currently proposed sorting in publicly available solar GIS tools (solar cadastre), which consider only one attribute such as the solar irradiation or the payback period. Knowing the limits of these sorting scenarios, a multicriteria approach is adopted and takes into account more impactful attributes in the urban actors’ choices. The ELECTRE TRI methodology is used for this sorting problem. Different decisional criteria related to energy, economy, historic buildings and heritage, the structure and superstructure states of the roof are identified and detailed. The method is then applied to a set of alternatives (roofs) in a district of Geneva. The sorting results show that the proposed multicriteria scenario better integrates the complexity of the urban environment and provides a more relevant information regarding the suitability of the roofs for PV integration.},
	urldate = {2025-02-24},
	journal = {Sustainable Cities and Society},
	author = {Thebault, Martin and Clivillé, Vincent and Berrah, Lamia and Desthieux, Gilles},
	month = sep,
	year = {2020},
	keywords = {Classification, ELECTRE TRI, Roofs sorting, Solar cities, Suitability, Urban photovoltaic},
	pages = {102259},
}

@article{mohajeri_city-scale_2018,
	title = {A city-scale roof shape classification using machine learning for solar energy applications},
	volume = {121},
	issn = {0960-1481},
	url = {https://www.sciencedirect.com/science/article/pii/S0960148117313009},
	doi = {10.1016/j.renene.2017.12.096},
	abstract = {Solar energy deployment through PV installations in urban areas depends strongly on the shape, size, and orientation of available roofs. Here we use a machine learning approach, Support Vector Machine (SVM) classification, to classify 10,085 building roofs in relation to their received solar energy in the city of Geneva in Switzerland. The SVM correctly identifies six types of roof shapes in 66\% of cases, that is, flat \& shed, gable, hip, gambrel \& mansard, cross/corner gable \& hip, and complex roofs. We classify the roofs based on their useful area for PV installations and potential for receiving solar energy. For most roof shapes, the ratio between useful roof area and building footprint area is close to one, suggesting that footprint is a good measure of useful PV roof area. The main exception is the gable where this ratio is 1.18. The flat and shed roofs have the second highest useful roof area for PV (complex roof being the highest) and the highest PV potential (in GWh). By contrast, hip roof has the lowest PV potential. Solar roof-shape classification provides basic information for designing new buildings, retrofitting interventions on the building roofs, and efficient solar integration on the roofs of buildings.},
	urldate = {2025-02-24},
	journal = {Renewable Energy},
	author = {Mohajeri, Nahid and Assouline, Dan and Guiboud, Berenice and Bill, Andreas and Gudmundsson, Agust and Scartezzini, Jean-Louis},
	month = jun,
	year = {2018},
	keywords = {Machine learning, PV potential, Roof shape classification, Support Vector Machine},
	pages = {81--93},
}

@misc{nacional_plan_nodate,
	title = {Plan {Nacional} de {Ortofotografía} {Aérea}},
	url = {https://pnoa.ign.es/},
	abstract = {Geoportal web del Plan Nacional de Ortofotografía Aérea PNOA},
	language = {es-ES},
	urldate = {2025-02-21},
	journal = {Plan Nacional de Ortofotografía Aérea},
	author = {Nacional, Instituto Geográfico, O. A. Centro Nacional de Información Geográfica},
}

@article{castello_deep_2019,
	title = {Deep learning in the built environment: automatic detection of rooftop solar panels using {Convolutional} {Neural} {Networks}},
	volume = {1343},
	issn = {1742-6588, 1742-6596},
	shorttitle = {Deep learning in the built environment},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1343/1/012034},
	doi = {10.1088/1742-6596/1343/1/012034},
	abstract = {Abstract
            Mapping the location and size of solar installations in urban areas can be a valuable input for policymakers and for investing in distributed energy infrastructures. Machine Learning techniques, combined with satellite and aerial imagery, allow to overcome the limitations of surveys and sparse databases in providing this mapping at large scale. In this paper we apply a supervised method based on convolutional neural networks to delineate rooftop solar panels and to detect their sizes by means of pixel-wise image segmentation. As input to the algorithm, we rely on high resolution aerial photos provided by the Swiss Federal Office of Topography. We explore different data augmentation and we vary network parameters in order to maximize model performance. Preliminary results show that we are able to automatically detect in test images the area of a set of solar panels at pixel level with an accuracy of about 0.94 and an Intersection over Union index of up to 0.64. The scalability of the trained model allows to predict the existing solar panels deployment at the Swiss national scale. The correlation with local environmental and socio-economic variables would allow to extract predictive models to foster future adoption of solar technology in urban areas.},
	number = {1},
	urldate = {2025-02-20},
	journal = {Journal of Physics: Conference Series},
	author = {Castello, Roberto and Roquette, Simon and Esguerra, Martin and Guerra, Adrian and Scartezzini, Jean-Louis},
	month = nov,
	year = {2019},
	pages = {012034},
}

@article{aslani_automatic_2022,
	title = {Automatic identification of utilizable rooftop areas in digital surface models for photovoltaics potential assessment},
	volume = {306},
	issn = {03062619},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261921013283},
	doi = {10.1016/j.apenergy.2021.118033},
	language = {en},
	urldate = {2025-02-20},
	journal = {Applied Energy},
	author = {Aslani, Mohammad and Seipel, Stefan},
	month = jan,
	year = {2022},
	pages = {118033},
}

@article{narjabadifam_framework_2022,
	title = {Framework for {Mapping} and {Optimizing} the {Solar} {Rooftop} {Potential} of {Buildings} in {Urban} {Systems}},
	volume = {15},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/15/5/1738},
	doi = {10.3390/en15051738},
	abstract = {The accurate prediction of the solar energy that can be generated using the rooftops of buildings is an essential tool for many researchers, decision makers, and investors for creating sustainable cities and societies. This study is focused on the development of an automated method to extract the useable areas of rooftops and optimize the solar PV panel layout based on the given electricity loading of a building. In this context, the authors of this article developed two crucial methods. First, a special pixel-based rooftop recognition methodology was developed to analyze detailed and complex rooftop types while avoiding the challenges associated with the nature of the particular building rooftops. Second, a multi-objective enveloped min–max optimization algorithm was developed to maximize solar energy generation and minimize energy cost in terms of payback based on the marginal price signals. This optimization algorithm facilitates the optimal integration of three controlled variables—tilt angle, azimuth angle, and inter-row spacing—under a non-linear optimization space. The performance of proposed algorithms is demonstrated using three campus buildings at the University of Alberta, Edmonton, Alberta, Canada as case studies. It is shown that the proposed algorithms can be used to optimize PV panel distribution while effectively maintaining system constraints.},
	language = {en},
	number = {5},
	urldate = {2025-02-20},
	journal = {Energies},
	author = {Narjabadifam, Nima and Al-Saffar, Mohammed and Zhang, Yongquan and Nofech, Joseph and Cen, Asdrubal Cheng and Awad, Hadia and Versteege, Michael and Gül, Mustafa},
	month = feb,
	year = {2022},
	pages = {1738},
}

@article{yang_green_2018,
	title = {Green and cool roofs’ urban heat island mitigation potential in tropical climate},
	volume = {173},
	issn = {0038092X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0038092X18307667},
	doi = {10.1016/j.solener.2018.08.006},
	language = {en},
	urldate = {2025-02-20},
	journal = {Solar Energy},
	author = {Yang, Junjing and Mohan Kumar, Devi Llamathy and Pyrgou, Andri and Chong, Adrian and Santamouris, Mat and Kolokotsa, Denia and Lee, Siew Eang},
	month = oct,
	year = {2018},
	pages = {597--609},
}

@article{ordonez_analysis_2010,
	title = {Analysis of the photovoltaic solar energy capacity of residential rooftops in {Andalusia} ({Spain})},
	volume = {14},
	issn = {1364-0321},
	url = {https://www.sciencedirect.com/science/article/pii/S136403211000002X},
	doi = {10.1016/j.rser.2010.01.001},
	abstract = {Fossil fuel energy resources are becoming increasingly scarce. Given the negative environmental impacts (e.g. greenhouse gas emissions) that accompany their use, it is hardly surprising that the development of renewable energies has become a major priority in the world today. Andalusia, with a mean solar radiation of 4.75kWh/m2 per day and a surface area of 87,597km2, is the region in Europe with the highest solar energy potential. This research study determined the solar energy potential in Andalusia for grid-connected photovoltaic systems installed on residential rooftops. A methodology was developed for this purpose, which first involved a description of building characteristics, followed by the calculation of the useful roof surface area where photovoltaic arrays could be installed. In the next phase of the study, the mean solar irradiation characteristics were defined as well as the technical parameters of the photovoltaic systems. All of these factors allowed us to estimate the amount of electricity that could be potentially generated per year by solar panels.},
	number = {7},
	urldate = {2025-02-21},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Ordóñez, J. and Jadraque, E. and Alegre, J. and Martínez, G.},
	month = sep,
	year = {2010},
	keywords = {Edification, Photovoltaic solar energy, Renewable energy, Solar energy},
	pages = {2122--2130},
}

@inproceedings{kurdi_automated_2019,
	address = {Perth, Australia},
	title = {Automated {Building} {Footprint} and {3D} {Building} {Model} {Generation} from {Lidar} {Point} {Cloud} {Data}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72813-857-2},
	url = {https://ieeexplore.ieee.org/document/8946008/},
	doi = {10.1109/DICTA47822.2019.8946008},
	urldate = {2025-02-20},
	booktitle = {2019 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	publisher = {IEEE},
	author = {Kurdi, Fayez Tarsha and Awrangjeb, Mohammad and Liew, Alan Wee-Chung},
	month = dec,
	year = {2019},
	pages = {1--8},
}

@article{el_merabet_building_2015,
	title = {Building {Roof} {Segmentation} from {Aerial} {Images} {Using} a {Lineand} {Region}-{Based} {Watershed} {Segmentation} {Technique}},
	volume = {15},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/15/2/3172},
	doi = {10.3390/s150203172},
	abstract = {In this paper, we present a novel strategy for roof segmentation from aerial images (orthophotoplans) based on the cooperation of edge- and region-based segmentation methods. The proposed strategy is composed of three major steps. The first one, called the pre-processing step, consists of simplifying the acquired image with an appropriate couple of invariant and gradient, optimized for the application, in order to limit illumination changes (shadows, brightness, etc.) affecting the images. The second step is composed of two main parallel treatments: on the one hand, the simplified image is segmented by watershed regions. Even if the first segmentation of this step provides good results in general, the image is often over-segmented. To alleviate this problem, an efficient region merging strategy adapted to the orthophotoplan particularities, with a 2D modeling of roof ridges technique, is applied. On the other hand, the simplified image is segmented by watershed lines. The third step consists of integrating both watershed segmentation strategies into a single cooperative segmentation scheme in order to achieve satisfactory segmentation results. Tests have been performed on orthophotoplans containing 100 roofs with varying complexity, and the results are evaluated with the VINETcriterion using ground-truth image segmentation. A comparison with five popular segmentation techniques of the literature demonstrates the effectiveness and the reliability of the proposed approach. Indeed, we obtain a good segmentation rate of 96\% with the proposed method compared to 87.5\% with statistical region merging (SRM), 84\% with mean shift, 82\% with color structure code (CSC), 80\% with efficient graph-based segmentation algorithm (EGBIS) and 71\% with JSEG.},
	language = {en},
	number = {2},
	urldate = {2025-02-20},
	journal = {Sensors},
	author = {El Merabet, Youssef and Meurie, Cyril and Ruichek, Yassine and Sbihi, Abderrahmane and Touahni, Raja},
	month = feb,
	year = {2015},
	pages = {3172--3203},
}

@misc{watanabe_tree-structured_2023,
	title = {Tree-{Structured} {Parzen} {Estimator}: {Understanding} {Its} {Algorithm} {Components} and {Their} {Roles} for {Better} {Empirical} {Performance}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Tree-{Structured} {Parzen} {Estimator}},
	url = {https://arxiv.org/abs/2304.11127},
	doi = {10.48550/ARXIV.2304.11127},
	abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Watanabe, Shuhei},
	year = {2023},
	note = {Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{liu_shadow_2022,
	title = {Shadow {Removal} from {UAV} {Images} {Based} on {Color} and {Texture} {Equalization} {Compensation} of {Local} {Homogeneous} {Regions}},
	volume = {14},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/11/2616},
	doi = {10.3390/rs14112616},
	abstract = {Due to imaging and lighting directions, shadows are inevitably formed in unmanned aerial vehicle (UAV) images. This causes shadowed regions with missed and occluded information, such as color and texture details. Shadow detection and compensation from remote sensing images is essential for recovering the missed information contained in these images. Current methods are mainly aimed at processing shadows with simple scenes. For UAV remote sensing images with a complex background and multiple shadows, problems inevitably occur, such as color distortion or texture information loss in the shadow compensation result. In this paper, we propose a novel shadow removal algorithm from UAV remote sensing images based on color and texture equalization compensation of local homogeneous regions. Firstly, the UAV imagery is split into blocks by selecting the size of the sliding window. The shadow was enhanced by a new shadow detection index (SDI) and threshold segmentation was applied to obtain the shadow mask. Then, the homogeneous regions are extracted with LiDAR intensity and elevation information. Finally, the information of the non-shadow objects of the homogeneous regions is used to restore the missed information in the shadow objects of the regions. The results revealed that the average overall accuracy of shadow detection is 98.23\% and the average F1 score is 95.84\%. The average color difference is 1.891, the average shadow standard deviation index is 15.419, and the average gradient similarity is 0.726. The results have shown that the proposed method performs well in both subjective and objective evaluations.},
	language = {en},
	number = {11},
	urldate = {2025-02-20},
	journal = {Remote Sensing},
	author = {Liu, Xiaoxia and Yang, Fengbao and Wei, Hong and Gao, Min},
	month = may,
	year = {2022},
	pages = {2616},
}

@article{qian_deep_2022,
	title = {Deep {Roof} {Refiner}: {A} detail-oriented deep learning network for refined delineation of roof structure lines using satellite imagery},
	volume = {107},
	issn = {15698432},
	shorttitle = {Deep {Roof} {Refiner}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030324342200006X},
	doi = {10.1016/j.jag.2022.102680},
	language = {en},
	urldate = {2025-02-20},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Qian, Zhen and Chen, Min and Zhong, Teng and Zhang, Fan and Zhu, Rui and Zhang, Zhixin and Zhang, Kai and Sun, Zhuo and Lü, Guonian},
	month = mar,
	year = {2022},
	pages = {102680},
}

@article{stowell_harmonised_2020,
	title = {A harmonised, high-coverage, open dataset of solar photovoltaic installations in the {UK}},
	volume = {7},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-020-00739-0},
	doi = {10.1038/s41597-020-00739-0},
	abstract = {Abstract
            Solar photovoltaic (PV) is an increasingly significant fraction of electricity generation. Efficient management, and innovations such as short-term forecasting and machine vision, demand high-resolution geographic datasets of PV installations. However, official and public sources have notable deficiencies: spatial imprecision, gaps in coverage and lack of crucial meta data, especially for small-scale solar panel installations. We present the results of a major crowd-sourcing campaign to create open geographic data for over 260,000 solar PV installations across the UK, covering an estimated 86\% of the capacity in the country. We focus in particular on capturing small-scale domestic solar PV, which accounts for a significant fraction of generation but was until now very poorly documented. Our dataset suggests nameplate capacities in the UK (as of September 2020) amount to a total of 10.66 GW explicitly mapped, or 13.93 GW when missing capacities are inferred. Our method is applied to the UK but applicable worldwide, and compatible with continual updating to track the rapid growth in PV deployment.},
	language = {en},
	number = {1},
	urldate = {2025-02-20},
	journal = {Scientific Data},
	author = {Stowell, Dan and Kelly, Jack and Tanner, Damien and Taylor, Jamie and Jones, Ethan and Geddes, James and Chalstrey, Ed},
	month = nov,
	year = {2020},
	pages = {394},
}

@misc{esri_quoi_2025,
	title = {A quoi correspondent les données lidar ?—{ArcGIS} {Pro} {\textbar} {Documentation}},
	url = {https://pro.arcgis.com/fr/pro-app/latest/help/data/las-dataset/what-is-lidar-.htm},
	language = {fr},
	urldate = {2025-02-20},
	journal = {A quoi correspondent les données lidar ?},
	author = {ESRI},
	year = {2025},
}

@article{zhong_spatial_2022,
	title = {A spatial optimization approach to increase the accuracy of rooftop solar energy assessments},
	volume = {316},
	issn = {03062619},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261922005062},
	doi = {10.1016/j.apenergy.2022.119128},
	language = {en},
	urldate = {2025-02-20},
	journal = {Applied Energy},
	author = {Zhong, Qing and Nelson, Jake R. and Tong, Daoqin and Grubesic, Tony H.},
	month = jun,
	year = {2022},
	pages = {119128},
}

@article{malof_automatic_2016,
	title = {Automatic detection of solar photovoltaic arrays in high resolution aerial imagery},
	volume = {183},
	issn = {03062619},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306261916313009},
	doi = {10.1016/j.apenergy.2016.08.191},
	language = {en},
	urldate = {2025-02-20},
	journal = {Applied Energy},
	author = {Malof, Jordan M. and Bradbury, Kyle and Collins, Leslie M. and Newell, Richard G.},
	month = dec,
	year = {2016},
	pages = {229--240},
}

@misc{herny_detection_2024,
	title = {Detection of occupied and free surfaces on rooftops},
	url = {https://tech.stdl.ch/PROJ-ROOFTOPS/},
	language = {en},
	urldate = {2025-02-20},
	author = {Herny, Clémence and Salamin, Gwenaëlle and Cerioni, Alessandro and Pott, Roxanne},
	month = jan,
	year = {2024},
}

@book{zahn_cours_2024,
	address = {Lucerne},
	title = {Cours {Deep} {Learning} ({TSM}\_DeLearn) {MSE}},
	language = {en},
	publisher = {Lucerne University of Applied Science},
	author = {Zahn, Klaus},
	year = {2024},
}

@article{li_solarnet_2023,
	title = {{SolarNet}: {A} convolutional neural network-based framework for rooftop solar potential estimation from aerial imagery},
	volume = {116},
	issn = {1569-8432},
	shorttitle = {{SolarNet}},
	url = {https://www.sciencedirect.com/science/article/pii/S1569843222002862},
	doi = {10.1016/j.jag.2022.103098},
	abstract = {Solar power is a clean and renewable energy source. Promoting solar technology can not only offer all people affordable, reliable, and modern energy, but also mitigate energy-related emissions and pollutants. This significantly contributes to sustainable development goals. Aerial imagery can provide a cost-effective way for large-scale rooftop solar potential analysis when compared to other data sources. Existing studies mainly utilize aerial imagery and convolutional neural networks to learn the roof segmentation mask or the rooftop geometry map, which are the preliminary input for rooftop solar potential estimation. However, these methods fail to achieve precise solar potential analysis results. To address this issue, we propose a framework, which is termed as SolarNet for rooftop solar potential estimation. A novel multi-task learning network is devised in SolarNet to learn our proposed novel representation for rooftop geometry that incorporates 6 roof segments and orientations. Specifically, this network first learns a roof segmentation map, and then together with the extracted multiscale and contextual features to learn a roof geometry map. Finally, the solar potential can be estimated from the learned roof geometry map. The effectiveness of SolarNet is validated on two datasets: DeepRoof and RID datasets. Experimental results demonstrate that SolarNet can improve not only rooftop geometry prediction accuracy but also solar potential estimation precision, which significantly outperforms other competitors.},
	urldate = {2025-02-20},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Li, Qingyu and Krapf, Sebastian and Shi, Yilei and Zhu, Xiao Xiang},
	month = feb,
	year = {2023},
	keywords = {Convolutional neural network, Remote sensing, Renewable energy, Roof segments and orientations, Solar potential},
	pages = {103098},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional
logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for
nets containing circles; and that for any logical expression satisfying
certain conditions, one can find a net behaving in the fashion it describes.
It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	journal = {BULLETIN OF MATHEMATICAL BIOPHYSICS},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
}

@misc{noauthor_neurone_2025,
	title = {Neurone},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Neurone&oldid=222448745},
	abstract = {Un neurone, ou une cellule nerveuse, est une cellule excitable constituant l'unité fonctionnelle de la base du système nerveux.
Les neurones assurent la transmission d'un signal bioélectrique appelé influx nerveux. Ils ont deux propriétés physiologiques : l'excitabilité, c'est-à-dire la capacité de répondre aux stimulations et de convertir celles-ci en impulsions nerveuses, et la conductivité, c'est-à-dire la capacité de transmettre les impulsions.
La compréhension du fonctionnement du neurone et du système nerveux, particulièrement chez l'homme, est l'objet d'étude des neurosciences. Ses applications recouvrent la médecine, la psychologie et l'informatique (via les réseaux de neurones artificiel).},
	language = {fr},
	urldate = {2025-02-18},
	journal = {Wikipédia},
	month = jan,
	year = {2025},
	note = {Page Version ID: 222448745},
}

@techreport{lee_ipcc_2023,
	title = {{IPCC}, 2023: {Climate} {Change} 2023: {Synthesis} {Report}. {Contribution} of {Working} {Groups} {I}, {II} and {III} to the {Sixth} {Assessment} {Report} of the {Intergovernmental} {Panel} on {Climate} {Change} [{Core} {Writing} {Team}, {H}. {Lee} and {J}. {Romero} (eds.)]. {IPCC}, {Geneva}, {Switzerland}.},
	shorttitle = {{IPCC}, 2023},
	url = {https://www.ipcc.ch/report/ar6/syr/},
	abstract = {The Synthesis Report (SYR) is a stand-alone synthesis of the most policy-relevant evidence from the scientific, technical, and socio-economic literature assessed in the Sixth Assessment Report (AR6) of the Intergovernmental Panel on Climate Change (IPCC). The SYR distils and integrates the main findings of the three reports of the Working Groups of the IPCC during the AR6, and the three AR6 Special Reports into a concise document. It consists of a Summary for Policymakers and a longer report.},
	language = {en},
	urldate = {2025-02-17},
	institution = {Intergovernmental Panel on Climate Change (IPCC)},
	author = {Calvin, Katherine and Dasgupta, Dipak and Krinner, Gerhard and Mukherji, Aditi and Thorne, Peter W. and Trisos, Christopher and Romero, José and Aldunce, Paulina and Barrett, Ko and Blanco, Gabriel and Cheung, William W.L. and Connors, Sarah and Denton, Fatima and Diongue-Niang, Aïda and Dodman, David and Garschagen, Matthias and Geden, Oliver and Hayward, Bronwyn and Jones, Christopher and Jotzo, Frank and Krug, Thelma and Lasco, Rodel and Lee, Yune-Yi and Masson-Delmotte, Valérie and Meinshausen, Malte and Mintenbeck, Katja and Mokssit, Abdalah and Otto, Friederike E.L. and Pathak, Minal and Pirani, Anna and Poloczanska, Elvira and Pörtner, Hans-Otto and Revi, Aromar and Roberts, Debra C. and Roy, Joyashree and Ruane, Alex C. and Skea, Jim and Shukla, Priyadarshi R. and Slade, Raphael and Slangen, Aimée and Sokona, Youba and Sörensson, Anna A. and Tignor, Melinda and Van Vuuren, Detlef and Wei, Yi-Ming and Winkler, Harald and Zhai, Panmao and Zommers, Zinta and Hourcade, Jean-Charles and Johnson, Francis X. and Pachauri, Shonali and Simpson, Nicholas P. and Singh, Chandni and Thomas, Adelle and Totin, Edmond and Arias, Paola and Bustamante, Mercedes and Elgizouli, Ismail and Flato, Gregory and Howden, Mark and Méndez-Vallejo, Carlos and Pereira, Joy Jacqueline and Pichs-Madruga, Ramón and Rose, Steven K. and Saheb, Yamina and Sánchez Rodríguez, Roberto and Ürge-Vorsatz, Diana and Xiao, Cunde and Yassaa, Noureddine and Alegría, Andrés and Armour, Kyle and Bednar-Friedl, Birgit and Blok, Kornelis and Cissé, Guéladio and Dentener, Frank and Eriksen, Siri and Fischer, Erich and Garner, Gregory and Guivarch, Céline and Haasnoot, Marjolijn and Hansen, Gerrit and Hauser, Mathias and Hawkins, Ed and Hermans, Tim and Kopp, Robert and Leprince-Ringuet, Noëmie and Lewis, Jared and Ley, Debora and Ludden, Chloé and Niamir, Leila and Nicholls, Zebedee and Some, Shreya and Szopa, Sophie and Trewin, Blair and Van Der Wijst, Kaj-Ivar and Winter, Gundula and Witting, Maximilian and Birt, Arlene and Ha, Meeyoung and Romero, José and Kim, Jinmi and Haites, Erik F. and Jung, Yonghun and Stavins, Robert and Birt, Arlene and Ha, Meeyoung and Orendain, Dan Jezreel A. and Ignon, Lance and Park, Semin and Park, Youngin and Reisinger, Andy and Cammaramo, Diego and Fischlin, Andreas and Fuglestvedt, Jan S. and Hansen, Gerrit and Ludden, Chloé and Masson-Delmotte, Valérie and Matthews, J.B. Robin and Mintenbeck, Katja and Pirani, Anna and Poloczanska, Elvira and Leprince-Ringuet, Noëmie and Péan, Clotilde},
	collaborator = {Lee, Hoesung},
	month = jul,
	year = {2023},
	doi = {10.59327/IPCC/AR6-9789291691647},
	note = {Edition: First},
}
